{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09d824b-5d14-4ab6-9536-480e85e0fcb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc47744-7eb5-4ff8-8e82-d85732b81792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3076a0f6-8ac4-4a49-a5d5-fc731437f571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a811518-62f6-4847-b233-c1673d815a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'augmented copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset using ImageFolder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmented copy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print class names to verify labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'augmented copy'"
     ]
    }
   ],
   "source": [
    "# Load dataset using ImageFolder\n",
    "data_dir = \"augmented copy\"\n",
    "train_set = datasets.ImageFolder(os.path.join(data_dir), transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "# Print class names to verify labels\n",
    "print(f\"Classes: {train_set.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a81b7-e745-4b04-ba62-e411133b88a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(dataset, size_even=6, classes=None):\n",
    "    \n",
    "    size_even = 6  # Default number of samples to plot\n",
    "    dataset = train_set  # Default dataset to use (assuming 'train_set')\n",
    "    fig, axs = plt.subplots(2, int(size_even // 2))  # Create subplots for the samples\n",
    "\n",
    "    if classes is None:\n",
    "        classes = tuple([n for n in range(len(dataset))])  # Default class labels\n",
    "\n",
    "    n_samples = len(dataset)  # Total number of samples in the dataset\n",
    "\n",
    "    # Randomly select samples\n",
    "    samples = np.random.randint(0, high=n_samples - 1, size=int(size_even))\n",
    "\n",
    "    row = 0\n",
    "    col = 0\n",
    "    for n, sample in enumerate(samples):\n",
    "        img = dataset[sample][0]  # Get the image from the dataset\n",
    "        label = \"y={}\".format(classes[int(dataset[sample][1])])  # Get the label for the image\n",
    "\n",
    "        col = n\n",
    "\n",
    "        # Determine the row and column index for the current sample\n",
    "        if n > size_even / 2 - 1:\n",
    "            row = 1\n",
    "            col = n - int(size_even / 2)\n",
    "\n",
    "        # Display the image and label in the corresponding subplot\n",
    "        axs[row, col].imshow(make_grid(img, normalize=True).permute(1, 2, 0))\n",
    "        axs[row, col].set_title(label)\n",
    "        axs[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f72598-d9e6-452b-bf76-c6e9159b7e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_image_patchs(patchs, seq=False):\n",
    "\n",
    "    if patchs.shape[-3] == 1:\n",
    "        patchs = patchs.repeat(1, 1, 3, 1, 1)  # Add 3 channel dimension if image is grayscale\n",
    "\n",
    "    N_patchs = patchs.shape[1]  # Number of image patches\n",
    "\n",
    "    plot_patchs = make_grid(patchs, normalize=True).permute(0, 2, 3, 1)  # Arrange patches for plotting\n",
    "\n",
    "    N_rows = int(np.sqrt(N_patchs))  # Number of rows for the grid plot\n",
    "\n",
    "    # Create subplots for the image patches\n",
    "    if seq:\n",
    "        fig, ax = plt.subplots(1, N_rows * N_rows, sharex='col', sharey='row', figsize=(25, 5))\n",
    "        fig.suptitle(\"Image as Sequence\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(N_rows, N_rows, sharex='col', sharey='row')\n",
    "        fig.suptitle(\"Image\")\n",
    "\n",
    "    i, j = 0, 0\n",
    "    for n in range(N_patchs):\n",
    "        if seq:\n",
    "            ax[n].imshow(plot_patchs[n])\n",
    "            ax[n].set_xlabel(str(n + 1))\n",
    "            ax[n].axes.xaxis.set_ticklabels([])\n",
    "            ax[n].axes.yaxis.set_ticklabels([])\n",
    "        else:\n",
    "            if n % N_rows == 0 and n != 0:\n",
    "                i += 1\n",
    "                j = 0\n",
    "\n",
    "            ax[i, j].imshow(plot_patchs[n])\n",
    "            ax[i, j].set_ylabel(str(n + 1))\n",
    "            ax[i, j].axes.xaxis.set_ticklabels([])\n",
    "            ax[i, j].axes.yaxis.set_ticklabels([])\n",
    "            j += 1\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139adbe1-5afb-4fac-8642-89f26dbd2e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_patchs_lables(X, y, label=True, p_show=0.5):\n",
    "    # Detach X from its computational graph if it's not a leaf node\n",
    "    if not (X.is_leaf):\n",
    "        X = X.detach()\n",
    "\n",
    "    # Get the shape of the input tensor X\n",
    "    if X.shape[0] == 49:\n",
    "        sample = X.shape[1]\n",
    "        patch = X.shape[0]  # Patch size or sequence length\n",
    "        dim = X.shape[2]\n",
    "    if X.shape[0] == 1000:\n",
    "        sample = X.shape[0]\n",
    "        patch = X.shape[1]\n",
    "        dim = X.shape[2]\n",
    "\n",
    "    # Reshape X and convert it to a numpy array\n",
    "    X = X.reshape(sample * patch, dim).numpy()\n",
    "\n",
    "    # Flatten the y tensor and convert it to a numpy array to label each sequence\n",
    "    colors = y.repeat(patch, 1).T.flatten().numpy()\n",
    "\n",
    "    # Perform t-SNE on the X data\n",
    "    X_ = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "    # Create a scatter plot of the t-SNE transformed X data with each class labeled via color\n",
    "    fig, ax = plt.subplots()\n",
    "    for color in np.unique(colors):\n",
    "        temp = colors == color\n",
    "        ax.scatter(X_[temp, 0], X_[temp, 1], label=color)\n",
    "\n",
    "    # If the label argument is set to True, add labels to the scatter plot\n",
    "    if label:\n",
    "        for i, x in enumerate(X_):\n",
    "            # Label plots with a probability of p_show\n",
    "            if np.random.binomial(1, p_show, 1).item() == 1:\n",
    "                ax.annotate(str(i % patch), (x[0], x[1]))\n",
    "\n",
    "    # Add a legend to the scatter plot\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c65280-cac8-465e-9b49-e6db9cf9f37e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155288a-150b-4a12-9361-9ac9b4e2fd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "it=iter(train_loader)\n",
    "image=next(it)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd4a2e-8868-4f99-a38e-f3889b891f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B, C, H, W = image.shape\n",
    "print(\"batch size: {}, number of channels: {}, height: {},  width: {}\".format( B, C, H, W ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b324e7-1bb4-4076-830d-8de1f9f93631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb9e59-c0ea-42cf-807f-307ff4cb3961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image[0][0].numpy(),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba58df8-70f9-4b14-abe8-3741d88e40c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_size=8\n",
    "\n",
    "n_patches=(H/patch_size)* (W/patch_size)\n",
    "n_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af6370-576a-4327-bd6f-f9badc4a54ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patches=img_to_patch(torch.unsqueeze(train_set[2][0],0), patch_size, flatten_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dc5a8-d93d-42be-806c-83f1fd81add5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19671fb9-f49d-408a-9ffc-933cf27c6987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "plot_image_patchs(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652d40b-7d88-4fef-b0b9-87eaa1c8c381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_image_patchs(patches, seq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933329b4-402d-4d2a-8c2f-4a4966fbe88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "            #source:https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6f8e5-60c5-405d-bb42-5cc53455e85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, image_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels * (patch_size ** 2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + self.num_patches, embed_dim))  # 1 for CLS token\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "        # Ensure positional embeddings match number of patches\n",
    "        x = x + self.pos_embedding[:, :T + 1]\n",
    "\n",
    "        # Apply Transformer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07120e-5c05-4851-8671-d09c7fa494d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "patch_size = 4\n",
    "num_channels = 3  # Since logos are usually in RGB format (3 channels)\n",
    "num_patches = 64  # Adjust based on your image size and patch_size\n",
    "num_classes = 2  # Real or Fake\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4d5d2-9d33-4507-853c-e455e3137381",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "patch_size = 16  # Example patch size\n",
    "model = VisionTransformer(\n",
    "    embed_dim=embed_dim, hidden_dim=hidden_dim, num_channels=num_channels,\n",
    "    num_heads=num_heads, num_layers=num_layers, num_classes=2,  # Set to 2 for 'real' and 'fake'\n",
    "    patch_size=patch_size, image_size=image_size, dropout=dropout\n",
    ")\n",
    "\n",
    "\n",
    "# Move model to MPS device (for Mac M1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f510f7-9cbf-4743-bd0f-02392e15ef22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b696a2f-08dd-46cd-80e3-62f3953c4058",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871ef60-cb5f-4857-91aa-0587beb95989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"logo_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416a276-64fa-489a-8b2e-d9a80c4d9ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"logo_classifier.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b7ee7-0e28-4f15-81dd-f3b84f778232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the image transformations (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),  # Resize the image to match model's expected input size\n",
    "    transforms.ToTensor(),                        # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))          # Normalize (ensure it matches training normalization)\n",
    "])\n",
    "\n",
    "# Function to make a prediction on a single image\n",
    "def predict_image(model, image_path, device):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure image is RGB\n",
    "    image = transform(image).unsqueeze(0)          # Apply transformations and add batch dimension\n",
    "\n",
    "    # Move image to device (GPU or CPU)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode and disable gradient calculation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Forward pass through the model\n",
    "        _, predicted = torch.max(output, 1)  # Get the predicted class (0 or 1)\n",
    "\n",
    "    return predicted.item()\n",
    "\n",
    "# Example usage\n",
    "image_path = \"Data/Fake/cepce.png\"  # Replace with the path to your logo image\n",
    "model = VisionTransformer(  # Make sure model parameters match your trained model\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_channels=num_channels,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,  # Should be 2 for 'real' and 'fake'\n",
    "    patch_size=patch_size,\n",
    "    image_size=image_size,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Load trained weights (optional, if you're using the pre-trained model)\n",
    "model.load_state_dict(torch.load(\"vit_logo_classifier.pth\"))\n",
    "\n",
    "# Move model to the device (MPS or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Make a prediction\n",
    "predicted_class = predict_image(model, image_path, device)\n",
    "\n",
    "# Map predicted class to label\n",
    "class_names = ['real', 'fake']  # Assuming 0 = 'real' and 1 = 'fake'\n",
    "print(f\"Predicted class: {class_names[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5d1e0-5b6a-4ca4-9028-57a9c046e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the image transformations (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),  # Ensure image size matches the training input size\n",
    "    transforms.ToTensor(),                        # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))          # Normalize (ensure this matches your training normalization)\n",
    "])\n",
    "\n",
    "# Load the trademarks dataset using ImageFolder\n",
    "data_dir = \"sample/test\"  # The directory containing 'real' and 'fake' subfolders\n",
    "test_dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Disable gradient computation during inference\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Load your trained model (make sure parameters match the trained model)\n",
    "model = VisionTransformer(\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_channels=num_channels,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,  # Should be 2 for 'real' and 'fake'\n",
    "    patch_size=patch_size,\n",
    "    image_size=image_size,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(\"logo_classifier.pth\"))\n",
    "\n",
    "# Move the model to the device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the accuracy on the test dataset\n",
    "accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Accuracy on the trademarks dataset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27794e5f-4c06-400c-bb6e-66ab74b3ae37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81a601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
